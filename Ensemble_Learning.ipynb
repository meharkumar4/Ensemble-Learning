{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning"
      ],
      "metadata": {
        "id": "KdGdQiInhQPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Can we use Bagging for regression problems?**\n",
        "\n",
        "   Yes, Bagging can be used for regression problems. It involves training multiple regression models on different subsets of the data and averaging their predictions to improve accuracy and reduce variance.\n",
        "\n",
        "2. **What is the difference between multiple model training and single model training?**\n",
        "\n",
        "   Single model training builds one model on the entire dataset, while multiple model training (ensemble methods) combines predictions from several models to improve accuracy, reduce variance, and increase robustness.\n",
        "\n",
        "3. **Explain the concept of feature randomness in Random Forest.**\n",
        "\n",
        "   Feature randomness in Random Forest means each decision tree uses a random subset of features when splitting nodes. This helps reduce correlation between trees, improving generalization and reducing overfitting.\n",
        "\n",
        "4. **What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "   The OOB score is an internal validation method in Bagging where each model is tested on the data samples not used during its training. It estimates model accuracy without needing a separate validation set.\n",
        "\n",
        "5. **How can you measure the importance of features in a Random Forest model?**\n",
        "\n",
        "   Feature importance in Random Forest is measured by observing how much each feature decreases impurity across all trees. It can also be estimated using permutation importance, which checks performance changes when features are randomly shuffled.\n",
        "\n",
        "6. **Explain the working principle of a Bagging Classifier.**\n",
        "\n",
        "   A Bagging Classifier trains multiple base models (usually decision trees) on different bootstrap samples of the training data. Final predictions are made by majority voting, reducing overfitting and increasing model stability.\n",
        "\n",
        "7. **How do you evaluate a Bagging Classifierâ€™s performance?**\n",
        "\n",
        "   Performance can be evaluated using accuracy, precision, recall, F1-score, ROC-AUC, or confusion matrix on test data. The OOB score is also useful for assessing model performance during training without a separate validation set.\n",
        "\n",
        "8. **How does a Bagging Regressor work?**\n",
        "\n",
        "   A Bagging Regressor trains multiple regressors on different bootstrap samples of the training data. The final output is the average of all individual predictions, reducing variance and improving model stability.\n",
        "\n",
        "9. **What is the main advantage of ensemble techniques?**\n",
        "\n",
        "   The main advantage is increased accuracy and robustness by combining multiple models. Ensemble techniques reduce overfitting, handle noisy data better, and generalize well compared to individual models.\n",
        "\n",
        "10. **What is the main challenge of ensemble methods?**\n",
        "\n",
        "    The main challenges include increased computational cost, complexity in implementation, difficulty in interpreting results, and potential overfitting if not properly tuned.\n",
        "\n",
        "11. **Explain the key idea behind ensemble techniques.**\n",
        "\n",
        "    Ensemble techniques combine predictions from multiple models to improve overall performance. By aggregating the strengths of each model and reducing individual errors, they create more accurate and reliable predictions.\n",
        "\n",
        "12. **What is a Random Forest Classifier?**\n",
        "\n",
        "    A Random Forest Classifier is an ensemble model that builds multiple decision trees using bagging and feature randomness. It makes classification decisions based on the majority vote from all trees.\n",
        "\n",
        "13. **What are the main types of ensemble techniques?**\n",
        "\n",
        "    The main types are Bagging (e.g., Random Forest), Boosting (e.g., AdaBoost, Gradient Boosting), and Stacking. Each method combines multiple models in different ways to improve prediction performance.\n",
        "\n",
        "14. **What is ensemble learning in machine learning?**\n",
        "\n",
        "    Ensemble learning is a technique that combines multiple models to solve a single problem. It aims to improve prediction accuracy, robustness, and reduce model bias or variance compared to using one model.\n",
        "\n",
        "15. **When should we avoid using ensemble methods?**\n",
        "\n",
        "    Avoid ensemble methods when interpretability is critical, computational resources are limited, or when the data is small and simple models already perform well. Ensembles can be overkill in such cases.\n",
        "\n",
        "16. **How does Bagging help in reducing overfitting?**\n",
        "\n",
        "    Bagging reduces overfitting by training models on different subsets of the data, which introduces variation. Aggregating their predictions averages out noise and variance, leading to better generalization on unseen data.\n",
        "\n",
        "17. **Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "    Random Forests are better because they reduce overfitting, increase accuracy, and are more stable. They aggregate results from multiple trees, making the final prediction less sensitive to noise in the data.\n",
        "\n",
        "18. **What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "    Bootstrap sampling creates random subsets of the training data with replacement. Each model is trained on a different sample, which helps introduce diversity, reduce overfitting, and improve model performance.\n",
        "\n",
        "19. **What are some real-world applications of ensemble techniques?**\n",
        "\n",
        "    Ensemble techniques are used in fraud detection, spam filtering, credit scoring, stock market prediction, image recognition, and medical diagnosis due to their high accuracy and reliability.\n",
        "\n",
        "20. **What is the difference between Bagging and Boosting?**\n",
        "\n",
        "    Bagging trains models independently on random subsets to reduce variance. Boosting trains models sequentially, each correcting the errors of the previous one, focusing more on bias reduction. Boosting is more sensitive to overfitting.\n"
      ],
      "metadata": {
        "id": "K5NfmJkdhVYf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyicpLkhi5lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "3pjTduVpi6RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ACOq5fbojYcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0TmGzleOk6rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "importances = rf_clf.feature_importances_\n",
        "print(\"Feature Importances:\", importances)\n"
      ],
      "metadata": {
        "id": "jDVYPuMdk-gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "tree_pred = tree_reg.predict(X_test)\n",
        "\n",
        "print(\"Random Forest R2:\", r2_score(y_test, rf_pred))\n",
        "print(\"Decision Tree R2:\", r2_score(y_test, tree_pred))\n"
      ],
      "metadata": {
        "id": "SEv5brXylBKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf_oob.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", rf_clf_oob.oob_score_)\n"
      ],
      "metadata": {
        "id": "Ee0rWtOhlEUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "print(\"Bagging SVM Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "MPSPYHR4lJFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    print(f\"{n} Trees Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "id": "0JNPZWS0lNdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "bagging_log = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
        "bagging_log.fit(X_train, y_train)\n",
        "y_proba = bagging_log.predict_proba(X_test)[:, 1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_proba))"
      ],
      "metadata": {
        "id": "fAhlG4yGlOJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train a Random Forest Regressor and analyze feature importance scores\n",
        "rf_reg.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", rf_reg.feature_importances_)\n"
      ],
      "metadata": {
        "id": "lv3sCPZmlRvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "bagging_acc = accuracy_score(y_test, bagging_model.predict(X_test))\n",
        "rf_acc = accuracy_score(y_test, rf_model.predict(X_test))\n",
        "\n",
        "print(\"Bagging Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)"
      ],
      "metadata": {
        "id": "JDJsxWNtlUL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "a0ZZ-SoxlXWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "for n in [5, 10, 20]:\n",
        "    model = BaggingRegressor(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    print(f\"{n} Estimators MSE:\", mean_squared_error(y_test, preds))\n"
      ],
      "metadata": {
        "id": "DQOl585Nloif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "misclassified = X_test[preds != y_test]\n",
        "print(\"Number of misclassified samples:\", len(misclassified))\n"
      ],
      "metadata": {
        "id": "E2o3ynrklqqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "tree.fit(X_train, y_train)\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, bagging.predict(X_test)))\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, tree.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "wvdc57xyls9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "epj1qf29lvVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=3)\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", accuracy_score(y_test, stack.predict(X_test)))"
      ],
      "metadata": {
        "id": "CIS6xkiPly3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. Train a Random Forest Classifier and print the top 5 most important features\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "importances = model.feature_importances_\n",
        "top_5 = sorted(zip(importances, range(len(importances))), reverse=True)[:5]\n",
        "print(\"Top 5 Feature Indices and Importances:\", top_5)"
      ],
      "metadata": {
        "id": "hbOwmFSAl1zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, preds, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, preds, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, preds, average='weighted'))"
      ],
      "metadata": {
        "id": "ghBO5_xMl4Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "for depth in [3, 5, 10, None]:\n",
        "    model = RandomForestClassifier(max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Max Depth {depth}: Accuracy = {acc}\")\n"
      ],
      "metadata": {
        "id": "ffhKwsPwl6dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "dt_model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "knn_model = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "knn_model.fit(X_train, y_train)\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, dt_model.predict(X_test)))\n",
        "print(\"KNN MSE:\", mean_squared_error(y_test, knn_model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "l7Nomknll8ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "if len(set(y)) == 2:\n",
        "    prob = model.predict_proba(X_test)[:, 1]\n",
        "    print(\"ROC AUC Score:\", roc_auc_score(y_test, prob))\n"
      ],
      "metadata": {
        "id": "fzIKWju2l-Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "print(\"Cross-Validation Scores:\", scores)\n",
        "print(\"Mean CV Score:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "BPfpurycmAT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "prec, rec, _ = precision_recall_curve(y_test, probs)\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i2Zn-KrEmCP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42))\n",
        "]\n",
        "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=3)\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack.predict(X_test)))"
      ],
      "metadata": {
        "id": "SbJyNEFDmEbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "for bootstrap in [True, False]:\n",
        "    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, bootstrap=bootstrap, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    print(f\"Bootstrap = {bootstrap} MSE:\", mean_squared_error(y_test, preds))"
      ],
      "metadata": {
        "id": "QeN_r2MwmGkU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}